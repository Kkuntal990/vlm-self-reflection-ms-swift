apiVersion: v1
kind: Pod
metadata:
  name: vlm-jupyter
spec:
  restartPolicy: Never
  containers:
  - name: jupyter-container
    image: ghcr.io/kkuntal990/ms-swift-qwen:latest
    command: ["sleep", "infinity"]

    resources:
      limits:
        nvidia.com/a100: "2"
        memory: "32Gi"
        cpu: "4"
      requests:
        nvidia.com/a100: "2"
        memory: "32G"
        cpu: "4"


    volumeMounts:
      - name: cache-volume
        mountPath: /cache
      - name: outputs-volume
        mountPath: /outputs
      - name: dev-shm
        mountPath: /dev/shm

    env:
    # HuggingFace token (optional for public models)
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token
          key: token
          optional: true

    # Cache directories (from scripts/env.sh)
    - name: HF_HOME
      value: "/cache/hf"
    - name: TRANSFORMERS_CACHE
      value: "/cache/hf/transformers"
    - name: HF_DATASETS_CACHE
      value: "/cache/hf/datasets"
    - name: TORCH_HOME
      value: "/cache/torch"
    - name: PIP_CACHE_DIR
      value: "/cache/pip"
    - name: OUTPUT_DIR
      value: "/outputs"

    # Model and dataset configuration
    - name: MODEL_ID
      value: "Qwen/Qwen3-8B"
    - name: DATASET_ID
      value: "tatsu-lab/alpaca#500"

    # Training hyperparameters
    - name: MAX_LEN
      value: "2048"
    - name: BATCH
      value: "1"
    - name: GRAD_ACC
      value: "16"
    - name: EPOCHS
      value: "1"
    - name: LR
      value: "2e-4"
    - name: LORA_RANK
      value: "8"
    - name: LORA_ALPHA
      value: "16"
    - name: USE_RAM_CACHE
      value: "true"  # Copy model to /dev/shm for faster loading

    # DDP configuration
    - name: NPROC
      value: "2"
    - name: MASTER_PORT
      value: "29501"
    - name: NCCL_DEBUG
      value: "WARN"
    - name: CUDA_DEVICE_MAX_CONNECTIONS
      value: "1"
    - name: MASTER_ADDR
      value: "localhost"

    # Python and CUDA settings
    - name: PYTHONUNBUFFERED
      value: "1"
    - name: PYTORCH_CUDA_ALLOC_CONF
      value: "expandable_segments:True"

  tolerations:
    # Tolerate nodes with issues (allows scheduling on nodes being debugged)
    - key: nautilus.io/issue
      operator: Exists
      effect: NoSchedule

    # Tolerate reserved nodes
    - key: nautilus.io/reservation
      operator: Exists
      effect: NoSchedule

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - NVIDIA-A100-SXM4-80GB

  volumes:
    - name: cache-volume
      persistentVolumeClaim:
        claimName: ms-swift-cache-pvc
    - name: outputs-volume
      persistentVolumeClaim:
        claimName: ms-swift-outputs-pvc
    - name: dev-shm
      emptyDir:
        medium: Memory
        sizeLimit: 20Gi
